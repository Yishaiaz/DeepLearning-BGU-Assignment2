1. In order to enable tf.function decorator, need to pay attention to work with Tensor
2. cuda? GPU for training
3. seed, reset seed for every experiment
4. train/validation/test method? N way?
5. hyperparameters - grid search? Bayesian search?
6. set hyperparameters search space
7. batch normalization
8. L2 regularization/dropout
9. learning rate decay
10. save/load models
11. threshold similarity
12. data augmentation
13. transfer learning/resnet for instance
14. loss method? binary cross-entropy, triplet loss, contrastive loss
15. similarity metric? euclidean distance, weighted distance?
16. model architecture ? related to the loss method
17. weight initialization
18. momentum
19. dataset preparation
20. EDA
21. keras - hold model as a field or extends from model
22.

# TODO
1. Siamese network - binary cross entropy based, generic for L2 and Dropout and other layers - Liad.
2. Load the dataset as normalized tensors - Yishaia.
3. Train validation split - ready for N-way validation - Yishaia.
4. Preprocess the testset for N way metric score - Yishaia.
5. Check for keras wrapped N-way functionality (via the built-in API) - Liad & Yishaia
6. Grid search parameter space function implementation- Liad.
7. Check how to expand keras evaluate step (i.e. like metric) - Yishaia.
8. Different learning rates schedulers.
9. Data augmentation - choose which one to implement: rotation (45), crop, noise (Salt&Pepper), saturation, chain [Salt&Pepper, crop center]
10. Transfer learning with ResNet.
11. Bayse search (hyper params).
12. EDA
13.